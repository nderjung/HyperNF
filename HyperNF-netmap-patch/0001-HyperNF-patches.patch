From 83aadbb4ace4ae39eb0d146a07c53f06ecbcc5d3 Mon Sep 17 00:00:00 2001
From: Alexander Jung <a.jung@lancs.ac.uk>
Date: Fri, 16 Nov 2018 03:25:26 +0000
Subject: [PATCH] HyperNF patches

---
 LINUX/bsd_glue.h             |  38 ++---
 LINUX/i40e_netmap_linux.h    | 154 +++++++++++++++++
 LINUX/netmap.mak.in          |   1 -
 LINUX/netmap_linux.c         |  29 +++-
 LINUX/xennet_lock.h          | 186 +++++++++++++++++++++
 sys/dev/netmap/netmap.c      |   1 +
 sys/dev/netmap/netmap_kern.h | 209 ++++++++++++++++++++---
 sys/dev/netmap/netmap_mem2.c | 388 ++++++++++++++++++++++++++++++++++++++-----
 sys/dev/netmap/netmap_mem2.h |   4 +
 sys/dev/netmap/netmap_vale.c |  91 ++--------
 sys/net/netmap.h             |   3 +-
 11 files changed, 936 insertions(+), 168 deletions(-)
 create mode 100644 LINUX/xennet_lock.h

diff --git a/LINUX/bsd_glue.h b/LINUX/bsd_glue.h
index 5716182..bcae558 100644
--- a/LINUX/bsd_glue.h
+++ b/LINUX/bsd_glue.h
@@ -64,6 +64,8 @@
 /*----- support for compiling on older versions of linux -----*/
 #include "netmap_linux_config.h"
 
+#include "xennet_lock.h"
+
 #ifndef dma_rmb
 #define dma_rmb() rmb()
 #endif /* dma_rmb */
@@ -219,8 +221,8 @@ struct thread;
 #define bzero(a, len)		memset(a, 0, len)
 
 /* Atomic variables. */
-#define NM_ATOMIC_TEST_AND_SET(p)	test_and_set_bit(0, (p))
-#define NM_ATOMIC_CLEAR(p)		clear_bit(0, (p))
+#define NM_ATOMIC_TEST_AND_SET(p)	(!atomic_cmpset_int((p), 0, 1))
+#define NM_ATOMIC_CLEAR(p)		atomic_store_rel_int((p), 0)
 
 #define NM_ATOMIC_SET(p, v)             atomic_set(p, v)
 #define NM_ATOMIC_INC(p)                atomic_inc(p)
@@ -322,23 +324,8 @@ int linux_netmap_set_channels(struct net_device *, struct ethtool_channels *);
  * We use spin_lock_irqsave() because we use the lock in the
  * (hard) interrupt context.
  */
-typedef struct {
-        spinlock_t      sl;
-        ulong           flags;
-} safe_spinlock_t;
-
-static inline void mtx_lock(safe_spinlock_t *m)
-{
-        spin_lock_irqsave(&(m->sl), m->flags);
-}
-
-static inline void mtx_unlock(safe_spinlock_t *m)
-{
-	ulong flags = *(volatile ulong *)&m->flags;
-        spin_unlock_irqrestore(&(m->sl), flags);
-}
 
-#define mtx_init(a, b, c, d)	spin_lock_init(&((a)->sl))
+#define mtx_init(a, b, c, d)	memset(&((a)->sl), 0, sizeof(rwticket))
 #define mtx_destroy(a)
 
 #define mtx_lock_spin(a)	mtx_lock(a)
@@ -349,16 +336,17 @@ static inline void mtx_unlock(safe_spinlock_t *m)
  * Must change to proper rwlock, and then can move the definitions
  * into the main netmap.c file.
  */
-#define BDG_RWLOCK_T		struct rw_semaphore
-#define BDG_RWINIT(b)		init_rwsem(&(b)->bdg_lock)
+#define BDG_RWLOCK_T		safe_spinlock_t
+#define BDG_RWINIT(b)		memset(b, 0, sizeof(safe_spinlock_t))
 #define BDG_RWDESTROY(b)
-#define BDG_WLOCK(b)		down_write(&(b)->bdg_lock)
-#define BDG_WUNLOCK(b)		up_write(&(b)->bdg_lock)
-#define BDG_RLOCK(b)		down_read(&(b)->bdg_lock)
-#define BDG_RUNLOCK(b)		up_read(&(b)->bdg_lock)
-#define BDG_RTRYLOCK(b)		down_read_trylock(&(b)->bdg_lock)
+#define BDG_WLOCK(b)		bdg_wrlock(&(b)->bdg_lock)
+#define BDG_WUNLOCK(b)		bdg_wrunlock(&(b)->bdg_lock)
+#define BDG_RLOCK(b)		bdg_rdlock(&(b)->bdg_lock)
+#define BDG_RUNLOCK(b)		bdg_rdunlock(&(b)->bdg_lock)
+#define BDG_RTRYLOCK(b)		bdg_rdtrylock(&(b)->bdg_lock)
 #define BDG_SET_VAR(lval, p)	((lval) = (p))
 #define BDG_GET_VAR(lval)	(lval)
+#define BDG_FREE(p)		xfree(p)
 
 #ifndef ilog2 /* not in 2.6.18 */
 static inline int ilog2(uint64_t n)
diff --git a/LINUX/i40e_netmap_linux.h b/LINUX/i40e_netmap_linux.h
index 226d690..3c792df 100644
--- a/LINUX/i40e_netmap_linux.h
+++ b/LINUX/i40e_netmap_linux.h
@@ -83,6 +83,155 @@ SYSCTL_INT(_dev_netmap, OID_AUTO, ix_rx_miss,
 SYSCTL_INT(_dev_netmap, OID_AUTO, ix_rx_miss_bufs,
     CTLFLAG_RW, &ix_rx_miss_bufs, 0, "potentially missed rx intr bufs");
 
+#define XENNET_IRQTYPE_I40E 1
+
+static void __xennet_unmap_i40e_sw(struct netmap_adapter *hwna, struct netmap_adapter *swna)
+{
+	if (netmap_xen_ops && netmap_xen_ops->unbind_hwsw) {
+		D("unbind hwsw");
+		netmap_xen_ops->unbind_hwsw(hwna, swna);
+	}
+	if (netmap_xen_ops && netmap_xen_ops->unmap) {
+		D("unmap netmap sw");
+		netmap_xen_ops->unmap(swna);
+	}
+}
+
+static int __xennet_map_i40e_sw(struct netmap_adapter *hwna, struct netmap_adapter *swna)
+{
+	int ret = 0;
+
+	if (netmap_xen_ops && netmap_xen_ops->map) {
+		D("try map netmap sw");
+		ret = netmap_xen_ops->map(swna, false, false);
+		if (ret < 0) {
+			D("failed to map netmap");
+			goto fail0;
+		}
+	}
+	if (netmap_xen_ops && netmap_xen_ops->bind_hwsw) {
+		ret = netmap_xen_ops->bind_hwsw(hwna, swna);
+		if (ret < 0) {
+			D("failed to bind hwsw");
+
+			goto fail1;
+		}
+	}
+
+	return ret;
+fail1:
+	if (netmap_xen_ops && netmap_xen_ops->unmap) {
+		D("unmap netmap sw");
+		netmap_xen_ops->unmap(swna);
+	}
+fail0:
+	return ret;
+}
+
+static void xennet_unmap_i40e(struct netmap_adapter *hwna)
+{
+	struct netmap_adapter *swna;
+
+	if (hwna->na_vp) {
+		swna = &hwna->na_vp->up;
+		__xennet_unmap_i40e_sw(hwna, swna);
+	}
+
+	if (netmap_xen_ops && netmap_xen_ops->unmap_i40e) {
+		D("unmap i40e");
+		netmap_xen_ops->unmap_i40e(hwna);
+	}
+
+	if (netmap_xen_ops && netmap_xen_ops->unmap) {
+		D("unmap netmap hw");
+		netmap_xen_ops->unmap(hwna);
+	}
+}
+
+static int xennet_map_i40e(struct netmap_adapter *hwna)
+{
+	struct netmap_adapter *swna;
+	struct ifnet *ifp = hwna->ifp;
+	struct i40e_netdev_priv *np = netdev_priv(ifp);
+	struct i40e_vsi *vsi = np->vsi;
+	struct i40e_pf *pf = (struct i40e_pf *)vsi->back;
+	struct i40e_ring* txr, *rxr;
+	size_t size_tx_ring = sizeof(struct i40e_ring);
+	int ret = 0, i;
+
+	if (netmap_xen_ops && netmap_xen_ops->map) {
+		D("try map netmap hw");
+		ret = netmap_xen_ops->map(hwna, false, true);
+		if (ret < 0) {
+			D("failed to map netmap");
+			goto out;
+		}
+	}
+
+	if (netmap_xen_ops && netmap_xen_ops->map_i40e) {
+		D("try map i40e");
+		ret = netmap_xen_ops->map_i40e(hwna,
+						pci_resource_start(pf->pdev, 0),
+						pci_resource_len(pf->pdev, 0));
+		if (ret < 0) {
+			D("failed to map i40e");
+			goto fail3;
+		}
+	}
+
+	if (netmap_xen_ops && netmap_xen_ops->map_i40e_ring) {
+		for (i = 0; i < hwna->num_tx_rings; i++) {
+			unsigned long *hung_detected = &vsi->q_vectors[i]->hung_detected;
+			txr = NM_I40E_TX_RING(vsi, i);
+			rxr = NM_I40E_RX_RING(vsi, i);
+			D("%d/%d: try map i40e ring base_queue: %d , queue_index: %d",
+					i,
+					hwna->num_tx_rings,
+					vsi->base_queue, txr->queue_index);
+			ret = netmap_xen_ops->map_i40e_ring(hwna, i, (void *) txr, size_tx_ring,
+							    (void *) rxr, size_tx_ring,
+							    &vsi->q_vectors[i]->tx,
+							    &vsi->q_vectors[i]->rx,
+							    &vsi->base_queue,
+							    &txr->queue_index,
+							    &vsi->base_vector,
+							    (u32 *) hung_detected,
+							    (u32 *) &vsi->state,
+							    (u64 *) &pf->flags);
+			if (ret < 0) {
+				D("failed to map i40e ring");
+				goto fail4;
+			}
+		}
+	}
+
+	if (hwna->na_vp) {
+		swna = &hwna->na_vp->up;
+		ret = __xennet_map_i40e_sw(hwna, swna);
+		if (ret < 0) {
+			D("failed to map i40e sw");
+			goto fail4;
+		}
+	} else {
+		D("This NIC is not attached to any bridge");
+	}
+
+	D("i40e mapping is done");
+	goto out;
+fail4:
+	if (netmap_xen_ops && netmap_xen_ops->unmap_i40e) {
+		D("unmap i40e");
+		netmap_xen_ops->unmap_i40e(hwna);
+	}
+fail3:
+	if (netmap_xen_ops && netmap_xen_ops->unmap) {
+		D("unmap netmap");
+		netmap_xen_ops->unmap(hwna);
+	}
+out:
+	return ret;
+}
+
 #if 0
 static void
 set_crcstrip(struct ixgbe_hw *hw, int onoff)
@@ -216,6 +365,7 @@ i40e_netmap_reg(struct netmap_adapter *na, int onoff)
 	//set_crcstrip(&adapter->hw, onoff);
 	/* enable or disable flags and callbacks in na and ifp */
 	if (onoff) {
+		xennet_unmap_i40e(na);
 		nm_set_native_flags(na);
 	} else {
 		nm_clear_native_flags(na);
@@ -223,6 +373,10 @@ i40e_netmap_reg(struct netmap_adapter *na, int onoff)
 	if (was_running) {
 		i40e_up(vsi);
 	}
+
+	if (onoff)
+		xennet_map_i40e(na);
+
 	//set_crcstrip(&adapter->hw, onoff); // XXX why twice ?
 
 	clear_bit(__I40E_CONFIG_BUSY, NM_I40E_STATE(pf));
diff --git a/LINUX/netmap.mak.in b/LINUX/netmap.mak.in
index ce71bc9..5eafe6e 100644
--- a/LINUX/netmap.mak.in
+++ b/LINUX/netmap.mak.in
@@ -20,7 +20,6 @@ EXTRA_CFLAGS += -Wno-unused-but-set-variable
 EXTRA_CFLAGS += $(foreach s,$(SUBSYS),-DCONFIG_NETMAP_$(shell echo $s|tr a-z- A-Z_))
 EXTRA_CFLAGS += $(if $(DEBUG),-g)
 
-
 # We use KSRC for the kernel configuration and sources.
 # If the sources are elsewhere, then use SRC to point to them.
 KSRC = @KSRC@
diff --git a/LINUX/netmap_linux.c b/LINUX/netmap_linux.c
index 6dd4153..2c0aa4f 100644
--- a/LINUX/netmap_linux.c
+++ b/LINUX/netmap_linux.c
@@ -1092,6 +1092,11 @@ linux_netmap_mmap(struct file *f, struct vm_area_struct *vma)
 		vma->vm_private_data = priv;
 		vma->vm_ops = &linux_netmap_mmap_ops;
 	}
+
+	if (netmap_xen_ops && netmap_xen_ops->map) {
+		netmap_xen_ops->map(priv->np_na, true, false);
+	}
+
 	return 0;
 }
 
@@ -1189,8 +1194,14 @@ static int
 linux_netmap_release(struct inode *inode, struct file *file)
 {
 	(void)inode;	/* UNUSED */
-	if (file->private_data)
+	if (file->private_data) {
+		struct netmap_priv_d *priv = file->private_data;
+		if (netmap_xen_ops && netmap_xen_ops->unmap) {
+			if (priv->np_na)
+				netmap_xen_ops->unmap(priv->np_na);
+		}
 		netmap_dtor(file->private_data);
+	}
 	return (0);
 }
 
@@ -2398,6 +2409,22 @@ EXPORT_SYMBOL(netmap_pipe_rxsync);	/* used by veth module */
 #endif /* WITH_PIPES */
 EXPORT_SYMBOL(netmap_verbose);
 
+EXPORT_SYMBOL(ifunit_ref);
+EXPORT_SYMBOL(if_rele);
+EXPORT_SYMBOL(netmap_ioctl);
+EXPORT_SYMBOL(netmap_dtor);
+EXPORT_SYMBOL(netmap_priv_new);
+EXPORT_SYMBOL(netmap_global_lock);
+EXPORT_SYMBOL(netmap_mem_get_info);
+EXPORT_SYMBOL(netmap_mem_ofstophys);
+EXPORT_SYMBOL(netmap_mem_if_offset);
+EXPORT_SYMBOL(nm_txsync_prologue);
+EXPORT_SYMBOL(nm_rxsync_prologue);
+struct netmap_xennet_ops *netmap_xen_ops = NULL;
+EXPORT_SYMBOL(netmap_xen_ops);
+EXPORT_SYMBOL(netmap_mem_private_shared_new);
+EXPORT_SYMBOL(netmap_vp_rxsync);
+
 MODULE_AUTHOR("http://info.iet.unipi.it/~luigi/netmap/");
 MODULE_DESCRIPTION("The netmap packet I/O framework");
 MODULE_LICENSE("Dual BSD/GPL"); /* the code here is all BSD. */
diff --git a/LINUX/xennet_lock.h b/LINUX/xennet_lock.h
new file mode 100644
index 0000000..b07bf31
--- /dev/null
+++ b/LINUX/xennet_lock.h
@@ -0,0 +1,186 @@
+/* Cited from
+ * http://locklessinc.com/articles/locks/
+ * Ticket read-write lock
+ *
+ * Reference: John Mellor-Crummey and Michael Scott,
+ * Scalable Reader-Writer Synchronization for Shared-Memory Multiprocessors
+ */
+
+#ifndef _XENNET_LOCK_H
+#define _XENNET_LOCK_H
+
+#define xn_atomic_xadd(P, V) __sync_fetch_and_add((P), (V))
+#define xn_cmpxchg(P, O, N) __sync_val_compare_and_swap((P), (O), (N))
+#define xn_atomic_inc(P) __sync_add_and_fetch((P), 1)
+#define xn_atomic_dec(P) __sync_add_and_fetch((P), -1) 
+#define xn_atomic_add(P, V) __sync_add_and_fetch((P), (V))
+#define xn_atomic_set_bit(P, V) __sync_or_and_fetch((P), 1<<(V))
+#define xn_atomic_clear_bit(P, V) __sync_and_and_fetch((P), ~(1<<(V)))
+
+typedef union rwticket rwticket;
+
+union rwticket
+{
+    u32 u;
+    u16 us;
+    __extension__ struct
+    {
+        u8 write;
+        u8 read;
+        u8 users;
+        u8 margin;
+    } s;
+};
+
+typedef struct {
+        rwticket    sl;
+        ulong        flags;
+} safe_spinlock_t;
+
+static void rwticket_wrlock(rwticket *l)
+{
+    u32 me = xn_atomic_xadd(&l->u, (1<<16));
+    u8 val = me >> 16;
+
+    while (val != l->s.write) cpu_relax();
+}
+
+static void rwticket_wrunlock(rwticket *l)
+{
+    rwticket t = *l;
+
+    barrier();
+
+    t.s.write++;
+    t.s.read++;
+
+    *(unsigned short *) l = t.us;
+}
+
+#if defined(__xennet__) || defined(XENNET_LOCK)
+static void rwticket_rdlock(rwticket *l)
+{
+    u32 me = xn_atomic_xadd(&l->u, (1<<16));
+    u8 val = me >> 16;
+
+    while (val != l->s.read) cpu_relax();
+    l->s.read++;
+}
+
+static void rwticket_rdunlock(rwticket *l)
+{
+    xn_atomic_inc(&l->s.write);
+}
+
+static int rwticket_rdtrylock(rwticket *l)
+{
+    u32 me = l->s.users;
+    u32 write = l->s.write;
+    u8 menew = me + 1;
+    u32 cmp = (me << 16) + (me << 8) + write;
+    u32 cmpnew = ((unsigned) menew << 16) + (menew << 8) + write;
+    l->s.margin = 0; // for sure
+
+    if (xn_cmpxchg(&l->u, cmp, cmpnew) == cmp) return 1;
+
+    return 0;
+}
+
+static inline void bdg_rdlock(safe_spinlock_t *m)
+{
+    rwticket_rdlock(&(m->sl));
+}
+
+static inline unsigned int bdg_rdtrylock(safe_spinlock_t *m)
+{
+    return rwticket_rdtrylock(&(m->sl));
+}
+
+static inline void bdg_rdunlock(safe_spinlock_t *m)
+{
+    rwticket_rdunlock(&(m->sl));
+}
+
+#endif
+
+/* irqsave */
+static inline unsigned long rwticket_wrlock_irqsave(rwticket *l)
+{
+    unsigned long flags;
+#if !defined(__xennet__)
+    local_irq_save(flags);
+    preempt_disable();
+#else
+    flags = 0;
+#endif
+    rwticket_wrlock(l);
+
+    return flags;
+}
+
+static inline void rwticket_wrunlock_irqrestore(rwticket *l, unsigned long flags)
+{
+    rwticket_wrunlock(l);
+#if !defined(__xennet__)
+    local_irq_restore(flags);
+    preempt_enable();
+#endif
+}
+
+static inline void bdg_wrlock(safe_spinlock_t *m)
+{
+    rwticket_wrlock(&(m->sl));
+}
+
+static inline void bdg_wrunlock(safe_spinlock_t *m)
+{
+    rwticket_wrunlock(&(m->sl));
+}
+
+static inline void mtx_lock(safe_spinlock_t *m)
+{
+    m->flags = rwticket_wrlock_irqsave(&(m->sl));
+}
+
+static inline void mtx_unlock(safe_spinlock_t *m)
+{
+    ulong flags = ACCESS_ONCE(m->flags);
+    rwticket_wrunlock_irqrestore(&(m->sl), flags);
+}
+
+/*
+ * lock functions for NM_ATOMIC_TEST_AND_SET and NM_ATOMIC_CLEAR
+ */
+
+#define MPLOCKED        "lock ; "
+
+/* Copied from FreeBSD */
+static __inline int
+atomic_cmpset_int(volatile int *dst, int expect, int src)
+{
+    u_char res;
+
+    __asm __volatile(
+    "       " MPLOCKED "            "
+    "       cmpxchgl %3,%1 ;        "
+    "       sete    %0 ;            "
+    "# atomic_cmpset_int"
+    : "=q" (res),                   /* 0 */
+      "+m" (*dst),                  /* 1 */
+      "+a" (expect)                 /* 2 */
+    : "r" (src)                     /* 3 */
+    : "memory", "cc");
+    return (res);
+}
+
+#define __compiler_membar()     __asm __volatile(" " : : : "memory")
+/* Copied from FreeBSD */
+static __inline void
+atomic_store_rel_int(volatile int *p, int v)
+{
+
+    __compiler_membar();
+    *p = v;
+}
+
+#endif
diff --git a/sys/dev/netmap/netmap.c b/sys/dev/netmap/netmap.c
index 9d0add4..d5d2f75 100644
--- a/sys/dev/netmap/netmap.c
+++ b/sys/dev/netmap/netmap.c
@@ -451,6 +451,7 @@ ports attached to the switch)
 
 #elif defined(linux)
 
+#define XENNET_LOCK
 #include "bsd_glue.h"
 
 #elif defined(__APPLE__)
diff --git a/sys/dev/netmap/netmap_kern.h b/sys/dev/netmap/netmap_kern.h
index 50a0e85..e001b59 100644
--- a/sys/dev/netmap/netmap_kern.h
+++ b/sys/dev/netmap/netmap_kern.h
@@ -157,7 +157,7 @@ struct hrtimer {
 /* See explanation in nm_os_generic_xmit_frame. */
 #define	GEN_TX_MBUF_IFP(m)	((struct ifnet *)skb_shinfo(m)->destructor_arg)
 
-#define NM_ATOMIC_T	volatile long unsigned int
+#define NM_ATOMIC_T	volatile int
 
 #define NM_MTX_T	struct mutex	/* OS-specific sleepable lock */
 #define NM_MTX_INIT(m)	mutex_init(&(m))
@@ -443,12 +443,28 @@ struct netmap_kring {
 	 */
 	uint64_t	last_reclaim;
 
+	void		*nmif;
+	void		*xen_domain;
+	uint16_t	xen_irq1;
+	uint16_t	xen_irq2;
+	uint16_t	evtchn_port;
+	struct netmap_ring	*xen_ring;
+	struct nm_bdg_fwd	*xen_nkr_ft;
+	uint32_t		*xen_nkr_leases;
+	int (*xen_nm_sync)(struct netmap_kring *kring, int flags);
+	int (*xen_nm_notify)(struct netmap_kring *kring, int flags);
+	int (*xen_vp_sync)(struct netmap_kring *kring, int flags);
 
-	NM_SELINFO_T	si;		/* poll/select wait queue */
-	NM_LOCK_T	q_lock;		/* protects kring and ring. */
-	NM_ATOMIC_T	nr_busy;	/* prevent concurrent syscalls */
+	uint32_t	ring_id;	/* kring identifier */
+	enum txrx	tx;		/* kind of ring (tx or rx) */
 
-	struct netmap_adapter *na;
+	/* while nkr_stopped is set, no new [tr]xsync operations can
+	 * be started on this kring.
+	 * This is used by netmap_disable_all_rings()
+	 * to find a synchronization point where critical data
+	 * structures pointed to by the kring can be added or removed
+	 */
+	volatile int nkr_stopped;
 
 	/* The following fields are for VALE switch support */
 	struct nm_bdg_fwd *nkr_ft;
@@ -457,13 +473,21 @@ struct netmap_kring {
 	uint32_t	nkr_hwlease;
 	uint32_t	nkr_lease_idx;
 
-	/* while nkr_stopped is set, no new [tr]xsync operations can
-	 * be started on this kring.
-	 * This is used by netmap_disable_all_rings()
-	 * to find a synchronization point where critical data
-	 * structures pointed to by the kring can be added or removed
-	 */
-	volatile int nkr_stopped;
+//#ifdef WITH_PIPES
+	struct netmap_kring *pipe;	/* if this is a pipe ring,
+					 * pointer to the other end
+					 */
+	struct netmap_ring *save_ring;	/* pointer to hidden rings
+					 * (see netmap_pipe.c for details)
+					 */
+//#endif /* WITH_PIPES */
+
+	NM_ATOMIC_T	nr_busy;	/* prevent concurrent syscalls */
+	NM_LOCK_T	q_lock;		/* protects kring and ring. */
+	//-------------------Shared------------------
+	NM_SELINFO_T	si;		/* poll/select wait queue */
+	char name[64];			/* diagnostic */
+	struct netmap_adapter *na;
 
 	/* Support for adapters without native netmap support.
 	 * On tx rings we preallocate an array of tx buffers
@@ -478,10 +502,6 @@ struct netmap_kring {
 
 	uint32_t	users;		/* existing bindings for this ring */
 
-	uint32_t	ring_id;	/* kring identifier */
-	enum txrx	tx;		/* kind of ring (tx or rx) */
-	char name[64];			/* diagnostic */
-
 	/* [tx]sync callback for this kring.
 	 * The default nm_kring_create callback (netmap_krings_create)
 	 * sets the nm_sync callback of each hardware tx(rx) kring to
@@ -496,16 +516,13 @@ struct netmap_kring {
 	int (*nm_sync)(struct netmap_kring *kring, int flags);
 	int (*nm_notify)(struct netmap_kring *kring, int flags);
 
-#ifdef WITH_PIPES
-	struct netmap_kring *pipe;	/* if this is a pipe ring,
-					 * pointer to the other end
-					 */
-#endif /* WITH_PIPES */
-
 #ifdef WITH_VALE
 	int (*save_notify)(struct netmap_kring *kring, int flags);
 #endif
 
+	struct task_struct *hwtsk;
+	NM_SELINFO_T hwwq;
+
 #ifdef WITH_MONITOR
 	/* array of krings that are monitoring this kring */
 	struct netmap_kring **monitors;
@@ -713,6 +730,7 @@ struct netmap_adapter {
 	/* Back reference to the parent ifnet struct. Used for
 	 * hardware ports (emulated netmap included). */
 	struct ifnet *ifp; /* adapter is ifp->if_softc */
+	struct ifnet *ifp2;
 
 	/*---- callbacks for this netmap adapter -----*/
 	/*
@@ -752,6 +770,7 @@ struct netmap_adapter {
 	 *      kring->nm_notify.
 	 *      Return values are the same as for netmap_rx_irq().
 	 */
+	int (*xennet_map)(struct netmap_adapter *);
 	void (*nm_dtor)(struct netmap_adapter *);
 
 	int (*nm_register)(struct netmap_adapter *, int onoff);
@@ -821,6 +840,8 @@ struct netmap_adapter {
 	/* Offset of ethernet header for each packet. */
 	u_int virt_hdr_len;
 
+	void *xen_be;
+
 	char name[64];
 };
 
@@ -1441,7 +1462,7 @@ struct netmap_bdg_ops {
 u_int netmap_bdg_learning(struct nm_bdg_fwd *ft, uint8_t *dst_ring,
 		struct netmap_vp_adapter *);
 
-#define	NM_BRIDGES		8	/* number of bridges */
+#define	NM_BRIDGES		64	/* number of bridges */
 #define	NM_BDG_MAXPORTS		254	/* up to 254 */
 #define	NM_BDG_BROADCAST	NM_BDG_MAXPORTS
 #define	NM_BDG_NOPORT		(NM_BDG_MAXPORTS+1)
@@ -1773,6 +1794,8 @@ netmap_idx_k2n(struct netmap_kring *kr, int idx)
 struct lut_entry {
 	void *vaddr;		/* virtual address. */
 	vm_paddr_t paddr;	/* physical address. */
+	void *xvaddr;		/* Xen virtual address. */
+	xen_pfn_t gfn;
 };
 #else /* linux & _WIN32 */
 /* dma-mapping in linux can assign a buffer a different address
@@ -1783,6 +1806,8 @@ struct lut_entry {
  */
 struct lut_entry {
 	void *vaddr;		/* virtual address. */
+	void *xvaddr;		/* Xen virtual address. */
+	xen_pfn_t gfn;
 };
 
 struct plut_entry {
@@ -2173,4 +2198,142 @@ void ptnet_nm_krings_delete(struct netmap_adapter *na);
 void ptnet_nm_dtor(struct netmap_adapter *na);
 #endif /* WITH_PTNETMAP_GUEST */
 
+/*
+ * system parameters (most of them in netmap_kern.h)
+ * NM_NAME	prefix for switch port names, default "vale"
+ * NM_BDG_MAXPORTS	number of ports
+ * NM_BRIDGES	max number of switches in the system.
+ *	XXX should become a sysctl or tunable
+ *
+ * Switch ports are named valeX:Y where X is the switch name and Y
+ * is the port. If Y matches a physical interface name, the port is
+ * connected to a physical device.
+ *
+ * Unlike physical interfaces, switch ports use their own memory region
+ * for rings and buffers.
+ * The virtual interfaces use per-queue lock instead of core lock.
+ * In the tx loop, we aggregate traffic in batches to make all operations
+ * faster. The batch size is bridge_batch.
+ */
+#define NM_BDG_MAXRINGS		16	/* XXX unclear how many. */
+#define NM_BDG_MAXSLOTS		4096	/* XXX same as above */
+#define NM_BRIDGE_RINGSIZE	1024	/* in the device */
+#define NM_BDG_HASH		1024	/* forwarding table entries */
+#define NM_BDG_BATCH		1024	/* entries in the forwarding buffer */
+#define NM_MULTISEG		64	/* max size of a chain of bufs */
+/* actual size of the tables */
+#define NM_BDG_BATCH_MAX	(NM_BDG_BATCH + NM_MULTISEG)
+/* NM_FT_NULL terminates a list of slots in the ft */
+#define NM_FT_NULL		NM_BDG_BATCH_MAX
+#define NM_BDG_MFS_DEFAULT	1514
+
+/*
+ * For each output interface, nm_bdg_q is used to construct a list.
+ * bq_len is the number of output buffers (we can have coalescing
+ * during the copy).
+ */
+struct nm_bdg_q {
+	uint16_t bq_head;
+	uint16_t bq_tail;
+	uint32_t bq_len;	/* number of buffers */
+};
+
+/* XXX revise this */
+struct nm_hash_ent {
+	uint64_t	mac;	/* the top 2 bytes are the epoch */
+	uint64_t	ports;
+};
+
+/*
+ * nm_bridge is a descriptor for a VALE switch.
+ * Interfaces for a bridge are all in bdg_ports[].
+ * The array has fixed size, an empty entry does not terminate
+ * the search, but lookups only occur on attach/detach so we
+ * don't mind if they are slow.
+ *
+ * The bridge is non blocking on the transmit ports: excess
+ * packets are dropped if there is no room on the output port.
+ *
+ * bdg_lock protects accesses to the bdg_ports array.
+ * This is a rw lock (or equivalent).
+ */
+struct nm_bridge {
+	/* XXX what is the proper alignment/layout ? */
+	BDG_RWLOCK_T	bdg_lock;	/* protects bdg_ports */
+	int		bdg_namelen;
+	uint32_t	bdg_active_ports; /* 0 means free */
+	char		bdg_basename[IFNAMSIZ];
+
+	/* Indexes of active ports (up to active_ports)
+	 * and all other remaining ports.
+	 */
+	uint8_t		bdg_port_index[NM_BDG_MAXPORTS];
+
+	/* the forwarding table, MAC+ports.
+	 * XXX should be changed to an argument to be passed to
+	 * the lookup function, and allocated on attach
+	 */
+	struct nm_hash_ent *ht; // allocated on attach
+	//struct nm_hash_ent ht[NM_BDG_HASH];
+
+	/*
+	 * The function to decide the destination port.
+	 * It returns either of an index of the destination port,
+	 * NM_BDG_BROADCAST to broadcast this packet, or NM_BDG_NOPORT not to
+	 * forward this packet.  ring_nr is the source ring index, and the
+	 * function may overwrite this value to forward this packet to a
+	 * different ring index.
+	 * This function must be set by netmap_bdg_ctl().
+	 */
+	struct netmap_bdg_ops bdg_ops;
+	struct netmap_bdg_ops xen_bdg_ops;
+
+	void *xen_bdg_ports[NM_BDG_MAXPORTS];
+//------------------------------------------------------------------- Shared
+
+	struct netmap_vp_adapter *bdg_ports[NM_BDG_MAXPORTS];
+
+#ifdef CONFIG_NET_NS
+	struct net *ns;
+#endif /* CONFIG_NET_NS */
+};
+
+typedef int (*map_fn_t)(struct netmap_adapter *, bool, bool);
+typedef void (*unmap_fn_t)(struct netmap_adapter *);
+typedef int (*attach_fn_t)(struct nm_bridge *, struct netmap_vp_adapter *);
+typedef void (*detach_fn_t)(struct netmap_vp_adapter *);
+typedef int (*bind_hwsw_fn_t)(struct netmap_adapter *, struct netmap_adapter *);
+typedef void (*unbind_hwsw_fn_t)(struct netmap_adapter *, struct netmap_adapter *);
+typedef int (*expose_irq_t)(struct netmap_adapter *na, unsigned int irq, unsigned int irqtype, unsigned int ring_id);
+typedef int (*unexpose_irq_t)(struct netmap_adapter *na, unsigned int irq, unsigned int irqtype, unsigned int ring_id);
+typedef int (*map_i40e_fn_t)(struct netmap_adapter *na,
+			     resource_size_t dev_hw_addr, resource_size_t resouce_size);
+typedef int (*map_i40e_ring_fn_t)(struct netmap_adapter *na, int rid, void *tx_ring, size_t size_tx_ring,
+				    void *rx_ring, size_t size_rx_ring,
+				    void *tx_rc, void *rx_rc,
+				    u16 *base_queue_ptr, u16 *queue_index_ptr,
+				    u32 *base_vector_ptr,
+				    u32 *hung_detected,
+				    u32 *state,
+				    u64 *flags);
+typedef void (*unmap_i40e_fn_t)(struct netmap_adapter *na);
+struct netmap_xennet_ops {
+	map_fn_t map;
+	unmap_fn_t unmap;
+	attach_fn_t bdg_attach;
+	detach_fn_t bdg_detach;
+	bind_hwsw_fn_t bind_hwsw;
+	unbind_hwsw_fn_t unbind_hwsw;
+	expose_irq_t expose_irq;
+	unexpose_irq_t unexpose_irq;
+	map_i40e_fn_t map_i40e;
+	map_i40e_ring_fn_t map_i40e_ring;
+	unmap_i40e_fn_t unmap_i40e;
+};
+
+extern struct netmap_xennet_ops *netmap_xen_ops;
+
+static int netmap_vp_txsync(struct netmap_kring *kring, int flags);
+int netmap_vp_rxsync(struct netmap_kring *kring, int flags);
+
 #endif /* _NET_NETMAP_KERN_H_ */
diff --git a/sys/dev/netmap/netmap_mem2.c b/sys/dev/netmap/netmap_mem2.c
index afb2c22..1c7ba41 100644
--- a/sys/dev/netmap/netmap_mem2.c
+++ b/sys/dev/netmap/netmap_mem2.c
@@ -70,7 +70,7 @@ MALLOC_DEFINE(M_NETMAP, "netmap", "Network memory map");
 #ifdef _WIN32_USE_SMALL_GENERIC_DEVICES_MEMORY
 #define NETMAP_BUF_MAX_NUM  8*4096      /* if too big takes too much time to allocate */
 #else
-#define NETMAP_BUF_MAX_NUM 20*4096*2	/* large machine */
+#define NETMAP_BUF_MAX_NUM 4096*8	/* large machine */
 #endif
 
 #define NETMAP_POOL_MAX_NAMSZ	32
@@ -150,6 +150,41 @@ struct netmap_mem_ops {
 	void (*nmd_rings_delete)(struct netmap_adapter *);
 };
 
+typedef uint16_t nm_memid_t;
+
+/*
+ * Shared info for netmap allocator
+ *
+ * Each allocator contains this structur as first netmap_if.
+ * In this way, we can share same details about allocator
+ * to the VM.
+ * Used in ptnetmap.
+ */
+struct netmap_mem_shared_info {
+#ifndef _WIN32
+        struct netmap_if up;	/* ends with a 0-sized array, which VSC does not like */
+#else /* !_WIN32 */
+	char up[sizeof(struct netmap_if)];
+#endif /* !_WIN32 */
+        uint64_t features;
+#define NMS_FEAT_BUF_POOL          0x0001
+#define NMS_FEAT_MEMSIZE           0x0002
+
+        uint32_t buf_pool_offset;
+        uint32_t buf_pool_objtotal;
+        uint32_t buf_pool_objsize;
+        uint32_t totalsize;
+};
+
+#define NMS_NAME        "nms_info"
+#define NMS_VERSION     1
+static const struct netmap_if nms_if_blueprint = {
+    .ni_name = NMS_NAME,
+    .ni_version = NMS_VERSION,
+    .ni_tx_rings = 0,
+    .ni_rx_rings = 0
+};
+
 struct netmap_mem_d {
 	NMA_LOCK_T nm_mtx;  /* protect the allocator */
 	u_int nm_totalsize; /* shorthand */
@@ -432,6 +467,21 @@ static struct netmap_obj_params netmap_min_priv_params[NETMAP_POOLS_NR] = {
 	},
 };
 
+static struct netmap_obj_params netmap_min_priv_shared_params[NETMAP_POOLS_NR] = {
+	[NETMAP_IF_POOL] = {
+		.size = 1024,
+		.num  = 2,
+	},
+	[NETMAP_RING_POOL] = {
+		.size = 5*PAGE_SIZE,
+		.num  = 4,
+	},
+	[NETMAP_BUF_POOL] = {
+		.size = 2048,
+		.num  = 4098,
+	},
+};
+
 
 /*
  * nm_mem is the memory allocator used for all physical interfaces
@@ -490,6 +540,36 @@ struct netmap_mem_d nm_mem = {	/* Our memory allocator. */
 	.name = "1"
 };
 
+extern struct netmap_mem_ops netmap_mem_private_shared_ops; /* forward */
+static const struct netmap_mem_d nm_blueprint_shared = {
+	.pools = {
+		[NETMAP_IF_POOL] = {
+			.name 	= "%s_if",
+			.objminsize = sizeof(struct netmap_if),
+			.objmaxsize = 4096,
+			.nummin     = 1,
+			.nummax	    = 100,
+		},
+		[NETMAP_RING_POOL] = {
+			.name 	= "%s_ring",
+			.objminsize = sizeof(struct netmap_ring),
+			.objmaxsize = 32*PAGE_SIZE,
+			.nummin     = 2,
+			.nummax	    = 1024,
+		},
+		[NETMAP_BUF_POOL] = {
+			.name	= "%s_buf",
+			.objminsize = 64,
+			.objmaxsize = 65536,
+			.nummin     = 4,
+			.nummax	    = 1000000, /* one million! */
+		},
+	},
+
+	.flags = NETMAP_MEM_PRIVATE,
+
+	.ops = &netmap_mem_private_shared_ops
+};
 
 /* blueprint for the private memory allocators */
 /* XXX clang is not happy about using name as a print format */
@@ -656,7 +736,7 @@ nm_alloc_lut(u_int nobj)
 	size_t n = sizeof(struct lut_entry) * nobj;
 	struct lut_entry *lut;
 #ifdef linux
-	lut = vmalloc(n);
+	lut = kzalloc(n, GFP_KERNEL);
 #else
 	lut = nm_os_malloc(n);
 #endif
@@ -668,7 +748,7 @@ nm_free_lut(struct lut_entry *lut, u_int objtotal)
 {
 	bzero(lut, sizeof(struct lut_entry) * objtotal);
 #ifdef linux
-	vfree(lut);
+	kfree(lut);
 #else
 	nm_os_free(lut);
 #endif
@@ -1300,6 +1380,8 @@ netmap_config_obj_allocator(struct netmap_obj_pool *p, u_int objtotal, u_int obj
 	return 0;
 }
 
+#include <xen/page.h>
+
 /* call with NMA_LOCK held */
 static int
 netmap_finalize_obj_allocator(struct netmap_obj_pool *p)
@@ -1326,6 +1408,15 @@ netmap_finalize_obj_allocator(struct netmap_obj_pool *p)
 	 * Allocate clusters, init pointers
 	 */
 
+	if (xen_pv_domain()) {
+		if (!xen_feature(XENFEAT_auto_translated_physmap))
+			D("Xen PV domain");
+		else
+			D("Xen HVM domain");
+	} else {
+		D("Baremetal");
+	}
+
 	n = p->_clustsize;
 	for (i = 0; i < (int)p->objtotal;) {
 		int lim = i + p->_clustentries;
@@ -1375,7 +1466,14 @@ netmap_finalize_obj_allocator(struct netmap_obj_pool *p)
 		for (; i < lim; i++, clust += p->_objsize) {
 			p->lut[i].vaddr = clust;
 #if !defined(linux) && !defined(_WIN32)
-			p->lut[i].paddr = vtophys(clust);
+			if (xen_pv_domain()) {
+				if (!xen_feature(XENFEAT_auto_translated_physmap))
+					p->lut[i].paddr = virt_to_machine(clust).maddr;
+				else
+					p->lut[i].paddr = vtophys(clust);
+			} else {
+				p->lut[i].paddr = vtophys(clust);
+			}
 #endif
 		}
 	}
@@ -1392,6 +1490,127 @@ clean:
 	return ENOMEM;
 }
 
+static void
+netmap_mem_reset_all(struct netmap_mem_d *nmd)
+{
+	int i;
+
+	if (netmap_verbose)
+		D("resetting %p", nmd);
+	for (i = 0; i < NETMAP_POOLS_NR; i++) {
+		netmap_reset_obj_allocator(&nmd->pools[i]);
+	}
+	nmd->flags  &= ~NETMAP_MEM_FINALIZED;
+}
+
+static int
+netmap_mem_init_shared_info(struct netmap_mem_d *nmd)
+{
+	struct netmap_mem_shared_info *nms_info;
+	ssize_t base;
+
+        /* Use the first slot in IF_POOL */
+	nms_info = netmap_if_malloc(nmd, sizeof(*nms_info));
+	if (nms_info == NULL) {
+	    return ENOMEM;
+	}
+
+	base = netmap_if_offset(nmd, nms_info);
+
+        memcpy(&nms_info->up, &nms_if_blueprint, sizeof(nms_if_blueprint));
+	nms_info->buf_pool_offset = nmd->pools[NETMAP_IF_POOL].memtotal + nmd->pools[NETMAP_RING_POOL].memtotal;
+	nms_info->buf_pool_objtotal = nmd->pools[NETMAP_BUF_POOL].objtotal;
+	nms_info->buf_pool_objsize = nmd->pools[NETMAP_BUF_POOL]._objsize;
+	nms_info->totalsize = nmd->nm_totalsize;
+	nms_info->features = NMS_FEAT_BUF_POOL | NMS_FEAT_MEMSIZE;
+
+	return 0;
+}
+
+static int
+netmap_mem_finalize_all(struct netmap_mem_d *nmd)
+{
+	int i;
+	if (nmd->flags & NETMAP_MEM_FINALIZED)
+		return 0;
+	nmd->lasterr = 0;
+	nmd->nm_totalsize = 0;
+	for (i = 0; i < NETMAP_POOLS_NR; i++) {
+		nmd->lasterr = netmap_finalize_obj_allocator(&nmd->pools[i]);
+		if (nmd->lasterr)
+			goto error;
+		nmd->nm_totalsize += nmd->pools[i].memtotal;
+	}
+	/* buffers 0 and 1 are reserved */
+	nmd->pools[NETMAP_BUF_POOL].objfree -= 2;
+	nmd->pools[NETMAP_BUF_POOL].bitmap[0] = ~3;
+	nmd->flags |= NETMAP_MEM_FINALIZED;
+
+	/* expose info to the ptnetmap guest */
+	nmd->lasterr = netmap_mem_init_shared_info(nmd);
+	if (nmd->lasterr)
+	        goto error;
+
+	if (netmap_verbose)
+		D("interfaces %d KB, rings %d KB, buffers %d MB",
+		    nmd->pools[NETMAP_IF_POOL].memtotal >> 10,
+		    nmd->pools[NETMAP_RING_POOL].memtotal >> 10,
+		    nmd->pools[NETMAP_BUF_POOL].memtotal >> 20);
+
+	if (netmap_verbose)
+		D("Free buffers: %d", nmd->pools[NETMAP_BUF_POOL].objfree);
+
+
+	return 0;
+error:
+	netmap_mem_reset_all(nmd);
+	return nmd->lasterr;
+}
+
+static void
+netmap_mem_private_delete(struct netmap_mem_d *nmd)
+{
+	if (nmd == NULL)
+		return;
+	if (netmap_verbose)
+		D("deleting %p", nmd);
+	if (nmd->active > 0)
+		D("bug: deleting mem allocator with active=%d!", nmd->active);
+	nm_mem_release_id(nmd);
+	if (netmap_verbose)
+		D("done deleting %p", nmd);
+	NMA_LOCK_DESTROY(nmd);
+	nm_os_free(nmd);
+}
+
+static int
+netmap_mem_private_config(struct netmap_mem_d *nmd)
+{
+	/* nothing to do, we are configured on creation
+ 	 * and configuration never changes thereafter
+ 	 */
+	return 0;
+}
+
+static int
+netmap_mem_private_finalize(struct netmap_mem_d *nmd)
+{
+	int err;
+	err = netmap_mem_finalize_all(nmd);
+	if (!err)
+		nmd->active++;
+	return err;
+
+}
+
+static void
+netmap_mem_private_deref(struct netmap_mem_d *nmd)
+{
+	if (--nmd->active <= 0)
+		netmap_mem_reset_all(nmd);
+}
+
+
 /* call with lock held */
 static int
 netmap_mem_params_changed(struct netmap_obj_params* p)
@@ -1408,19 +1627,6 @@ netmap_mem_params_changed(struct netmap_obj_params* p)
 	return rv;
 }
 
-static void
-netmap_mem_reset_all(struct netmap_mem_d *nmd)
-{
-	int i;
-
-	if (netmap_verbose)
-		D("resetting %p", nmd);
-	for (i = 0; i < NETMAP_POOLS_NR; i++) {
-		netmap_reset_obj_allocator(&nmd->pools[i]);
-	}
-	nmd->flags  &= ~NETMAP_MEM_FINALIZED;
-}
-
 static int
 netmap_mem_unmap(struct netmap_obj_pool *p, struct netmap_adapter *na)
 {
@@ -1519,40 +1725,123 @@ netmap_mem_map(struct netmap_obj_pool *p, struct netmap_adapter *na)
 	return error;
 }
 
-static int
-netmap_mem_finalize_all(struct netmap_mem_d *nmd)
+static void
+netmap_mem_private_shared_delete(struct netmap_mem_d *nmd)
 {
-	int i;
-	if (nmd->flags & NETMAP_MEM_FINALIZED)
-		return 0;
-	nmd->lasterr = 0;
-	nmd->nm_totalsize = 0;
-	for (i = 0; i < NETMAP_POOLS_NR; i++) {
-		nmd->lasterr = netmap_finalize_obj_allocator(&nmd->pools[i]);
-		if (nmd->lasterr)
-			goto error;
-		nmd->nm_totalsize += nmd->pools[i].memtotal;
+	int last;
+
+	if (nmd == NULL)
+		return;
+	if (netmap_verbose)
+		D("deleting %p", nmd);
+
+	NMA_LOCK(nmd);
+	last = (--nmd->refcount == 0);
+	NM_DBG_REFC(nmd, func, line);
+	NMA_UNLOCK(nmd);
+
+	if (!last) {
+		D("There are still users %u", last);
+		return;
 	}
-	nmd->lasterr = netmap_mem_init_bitmaps(nmd);
-	if (nmd->lasterr)
-		goto error;
+	D("Delete nm_mem");
 
-	nmd->flags |= NETMAP_MEM_FINALIZED;
+	if (nmd->active > 0)
+		D("bug: deleting mem allocator with active=%d!", nmd->active);
 
+	nm_mem_release_id(nmd);
 	if (netmap_verbose)
-		D("interfaces %d KB, rings %d KB, buffers %d MB",
-		    nmd->pools[NETMAP_IF_POOL].memtotal >> 10,
-		    nmd->pools[NETMAP_RING_POOL].memtotal >> 10,
-		    nmd->pools[NETMAP_BUF_POOL].memtotal >> 20);
+		D("done deleting %p", nmd);
+	NMA_LOCK_DESTROY(nmd);
+	nm_os_free(nmd);
+}
 
-	if (netmap_verbose)
-		D("Free buffers: %d", nmd->pools[NETMAP_BUF_POOL].objfree);
+struct netmap_mem_d *
+netmap_mem_private_shared_new(const char *name, u_int txr, u_int txd,
+	u_int rxr, u_int rxd, u_int extra_bufs, u_int npipes, int *perr)
+{
+	struct netmap_mem_d *d = NULL;
+	struct netmap_obj_params p[NETMAP_POOLS_NR];
+	int i, err;
+	u_int v, maxd;
 
+	d = nm_os_malloc(sizeof(struct netmap_mem_d));
+	if (d == NULL) {
+		err = ENOMEM;
+		goto error;
+	}
 
-	return 0;
+	*d = nm_blueprint_shared;
+
+	err = nm_mem_assign_id(d);
+	if (err)
+		goto error;
+
+	/* account for the fake host rings */
+	txr++;
+	rxr++;
+
+	/* copy the min values */
+	for (i = 0; i < NETMAP_POOLS_NR; i++) {
+		p[i] = netmap_min_priv_shared_params[i];
+	}
+
+	/* possibly increase them to fit user request */
+	v = sizeof(struct netmap_if) + sizeof(ssize_t) * (txr + rxr);
+	if (p[NETMAP_IF_POOL].size < v)
+		p[NETMAP_IF_POOL].size = v;
+	v = 2 + 4 * npipes;
+	if (p[NETMAP_IF_POOL].num < v)
+		p[NETMAP_IF_POOL].num = v;
+	maxd = (txd > rxd) ? txd : rxd;
+	v = sizeof(struct netmap_ring) + sizeof(struct netmap_slot) * maxd;
+	if (p[NETMAP_RING_POOL].size < v)
+		p[NETMAP_RING_POOL].size = v;
+	/* each pipe endpoint needs two tx rings (1 normal + 1 host, fake)
+         * and two rx rings (again, 1 normal and 1 fake host)
+         */
+	v = txr + rxr + 8 * npipes;
+	if (p[NETMAP_RING_POOL].num < v)
+		p[NETMAP_RING_POOL].num = v;
+	/* for each pipe we only need the buffers for the 4 "real" rings.
+         * On the other end, the pipe ring dimension may be different from
+         * the parent port ring dimension. As a compromise, we allocate twice the
+         * space actually needed if the pipe rings were the same size as the parent rings
+         */
+	v = (4 * npipes + rxr) * rxd + (4 * npipes + txr) * txd + 2 + extra_bufs;
+		/* the +2 is for the tx and rx fake buffers (indices 0 and 1) */
+	if (p[NETMAP_BUF_POOL].num < v)
+		p[NETMAP_BUF_POOL].num = v;
+
+	//if (netmap_verbose)
+		D("req if %d*%d ring %d*%d buf %d*%d",
+			p[NETMAP_IF_POOL].num,
+			p[NETMAP_IF_POOL].size,
+			p[NETMAP_RING_POOL].num,
+			p[NETMAP_RING_POOL].size,
+			p[NETMAP_BUF_POOL].num,
+			p[NETMAP_BUF_POOL].size);
+
+	for (i = 0; i < NETMAP_POOLS_NR; i++) {
+		snprintf(d->pools[i].name, NETMAP_POOL_MAX_NAMSZ,
+				nm_blueprint.pools[i].name,
+				name);
+		err = netmap_config_obj_allocator(&d->pools[i],
+				p[i].num, p[i].size);
+		if (err)
+			goto error;
+	}
+
+	d->flags &= ~NETMAP_MEM_FINALIZED;
+
+	NMA_LOCK_INIT(d);
+
+	return d;
 error:
-	netmap_mem_reset_all(nmd);
-	return nmd->lasterr;
+	netmap_mem_private_shared_delete(d);
+	if (perr)
+		*perr = err;
+	return NULL;
 }
 
 /*
@@ -2013,6 +2302,21 @@ struct netmap_mem_ops netmap_mem_global_ops = {
 	.nmd_rings_delete = netmap_mem2_rings_delete
 };
 
+struct netmap_mem_ops netmap_mem_private_shared_ops = {
+	.nmd_get_lut = netmap_mem2_get_lut,
+	.nmd_get_info = netmap_mem2_get_info,
+	.nmd_ofstophys = netmap_mem2_ofstophys,
+	.nmd_config = netmap_mem_private_config,
+	.nmd_finalize = netmap_mem_private_finalize,
+	.nmd_deref = netmap_mem_private_deref,
+	.nmd_if_offset = netmap_mem2_if_offset,
+	.nmd_delete = netmap_mem_private_shared_delete,
+	.nmd_if_new = netmap_mem2_if_new,
+	.nmd_if_delete = netmap_mem2_if_delete,
+	.nmd_rings_create = netmap_mem2_rings_create,
+	.nmd_rings_delete = netmap_mem2_rings_delete
+};
+
 int
 netmap_mem_pools_info_get(struct nmreq *nmr, struct netmap_mem_d *nmd)
 {
diff --git a/sys/dev/netmap/netmap_mem2.h b/sys/dev/netmap/netmap_mem2.h
index 9723385..d3cb3c0 100644
--- a/sys/dev/netmap/netmap_mem2.h
+++ b/sys/dev/netmap/netmap_mem2.h
@@ -140,6 +140,10 @@ int	   netmap_mem_get_info(struct netmap_mem_d *, uint64_t *size, u_int *memflag
 ssize_t    netmap_mem_if_offset(struct netmap_mem_d *, const void *vaddr);
 struct netmap_mem_d* netmap_mem_private_new( u_int txr, u_int txd, u_int rxr, u_int rxd,
 		u_int extra_bufs, u_int npipes, int* error);
+struct netmap_mem_d* netmap_mem_private_shared_new(const char *name,
+		u_int txr, u_int txd, u_int rxr, u_int rxd, u_int extra_bufs, u_int npipes,
+		int* error);
+static void netmap_mem_private_shared_delete(struct netmap_mem_d *nmd);
 void	   netmap_mem_delete(struct netmap_mem_d *);
 
 #define netmap_mem_get(d) __netmap_mem_get(d, __FUNCTION__, __LINE__)
diff --git a/sys/dev/netmap/netmap_vale.c b/sys/dev/netmap/netmap_vale.c
index 41ebcee..e6086ab 100644
--- a/sys/dev/netmap/netmap_vale.c
+++ b/sys/dev/netmap/netmap_vale.c
@@ -95,6 +95,7 @@ __FBSDID("$FreeBSD: head/sys/dev/netmap/netmap.c 257176 2013-10-26 17:58:36Z gle
 
 #elif defined(linux)
 
+#define XENNET_LOCK
 #include "bsd_glue.h"
 
 #elif defined(__APPLE__)
@@ -169,73 +170,6 @@ static int netmap_vp_create(struct nmreq *, struct ifnet *,
 static int netmap_vp_reg(struct netmap_adapter *na, int onoff);
 static int netmap_bwrap_reg(struct netmap_adapter *, int onoff);
 
-/*
- * For each output interface, nm_bdg_q is used to construct a list.
- * bq_len is the number of output buffers (we can have coalescing
- * during the copy).
- */
-struct nm_bdg_q {
-	uint16_t bq_head;
-	uint16_t bq_tail;
-	uint32_t bq_len;	/* number of buffers */
-};
-
-/* XXX revise this */
-struct nm_hash_ent {
-	uint64_t	mac;	/* the top 2 bytes are the epoch */
-	uint64_t	ports;
-};
-
-/*
- * nm_bridge is a descriptor for a VALE switch.
- * Interfaces for a bridge are all in bdg_ports[].
- * The array has fixed size, an empty entry does not terminate
- * the search, but lookups only occur on attach/detach so we
- * don't mind if they are slow.
- *
- * The bridge is non blocking on the transmit ports: excess
- * packets are dropped if there is no room on the output port.
- *
- * bdg_lock protects accesses to the bdg_ports array.
- * This is a rw lock (or equivalent).
- */
-struct nm_bridge {
-	/* XXX what is the proper alignment/layout ? */
-	BDG_RWLOCK_T	bdg_lock;	/* protects bdg_ports */
-	int		bdg_namelen;
-	uint32_t	bdg_active_ports; /* 0 means free */
-	char		bdg_basename[IFNAMSIZ];
-
-	/* Indexes of active ports (up to active_ports)
-	 * and all other remaining ports.
-	 */
-	uint8_t		bdg_port_index[NM_BDG_MAXPORTS];
-
-	struct netmap_vp_adapter *bdg_ports[NM_BDG_MAXPORTS];
-
-
-	/*
-	 * The function to decide the destination port.
-	 * It returns either of an index of the destination port,
-	 * NM_BDG_BROADCAST to broadcast this packet, or NM_BDG_NOPORT not to
-	 * forward this packet.  ring_nr is the source ring index, and the
-	 * function may overwrite this value to forward this packet to a
-	 * different ring index.
-	 * This function must be set by netmap_bdg_ctl().
-	 */
-	struct netmap_bdg_ops bdg_ops;
-
-	/* the forwarding table, MAC+ports.
-	 * XXX should be changed to an argument to be passed to
-	 * the lookup function
-	 */
-	struct nm_hash_ent *ht; // allocated on attach
-
-#ifdef CONFIG_NET_NS
-	struct net *ns;
-#endif /* CONFIG_NET_NS */
-};
-
 const char*
 netmap_bdg_name(struct netmap_vp_adapter *vp)
 {
@@ -497,8 +431,10 @@ netmap_bdg_detach_common(struct nm_bridge *b, int hw, int sw)
 	if (b->bdg_ops.dtor)
 		b->bdg_ops.dtor(b->bdg_ports[s_hw]);
 	b->bdg_ports[s_hw] = NULL;
+	b->xen_bdg_ports[s_hw] = NULL;
 	if (s_sw >= 0) {
 		b->bdg_ports[s_sw] = NULL;
+		b->xen_bdg_ports[s_sw] = NULL;
 	}
 	memcpy(b->bdg_port_index, tmp, sizeof(tmp));
 	b->bdg_active_ports = lim;
@@ -1487,10 +1423,10 @@ nm_bdg_preflush(struct netmap_kring *kring, u_int end)
 	 * attached to a user process) or with a trylock otherwise (NICs).
 	 */
 	ND("wait rlock for %d packets", ((j > end ? lim+1 : 0) + end) - j);
-	if (na->up.na_flags & NAF_BDG_MAYSLEEP)
+	//if (na->up.na_flags & NAF_BDG_MAYSLEEP)
 		BDG_RLOCK(b);
-	else if (!BDG_RTRYLOCK(b))
-		return j;
+	//else if (!BDG_RTRYLOCK(b))
+	//	return j;
 	ND(5, "rlock acquired for %d packets", ((j > end ? lim+1 : 0) + end) - j);
 	ft = kring->nkr_ft;
 
@@ -1934,7 +1870,8 @@ nm_bdg_flush(struct nm_bdg_fwd *ft, u_int n, struct netmap_vp_adapter *na,
 
 		ND(5, "pass 2 dst %d is %x %s",
 			i, d_i, is_vp ? "virtual" : "nic/host");
-		dst_nr = d_i & (NM_BDG_MAXRINGS-1);
+		//dst_nr = d_i & (NM_BDG_MAXRINGS-1);
+		dst_nr = (smp_processor_id()+1) & (NM_BDG_MAXRINGS-1);
 		nrings = dst_na->up.num_rx_rings;
 		if (dst_nr >= nrings)
 			dst_nr = dst_nr % nrings;
@@ -2202,7 +2139,7 @@ done:
  * but we must acquire the queue's lock to protect against
  * writers on the same queue.
  */
-static int
+int
 netmap_vp_rxsync(struct netmap_kring *kring, int flags)
 {
 	int n;
@@ -2298,12 +2235,16 @@ netmap_vp_create(struct nmreq *nmr, struct ifnet *ifp,
 	na->nm_krings_delete = netmap_vp_krings_delete;
 	na->nm_dtor = netmap_vp_dtor;
 	D("nr_arg2 %d", nmr->nr_arg2);
-	na->nm_mem = nmd ?
-		netmap_mem_get(nmd):
-		netmap_mem_private_new(
+	if (nmr->nm_mem) {
+		D("We share the same nm_mem %p", nmr->nm_mem);
+		na->nm_mem = (struct netmap_mem_d *) nmr->nm_mem;
+		netmap_mem_get(na->nm_mem);
+	} else {
+		na->nm_mem = netmap_mem_private_new(
 			na->num_tx_rings, na->num_tx_desc,
 			na->num_rx_rings, na->num_rx_desc,
 			nmr->nr_arg3, npipes, &error);
+	}
 	if (na->nm_mem == NULL)
 		goto err;
 	na->nm_bdg_attach = netmap_vp_bdg_attach;
diff --git a/sys/net/netmap.h b/sys/net/netmap.h
index 78c0026..05325da 100644
--- a/sys/net/netmap.h
+++ b/sys/net/netmap.h
@@ -363,7 +363,7 @@ struct netmap_if {
 	 * The area is filled up by the kernel on NIOCREGIF,
 	 * and then only read by userspace code.
 	 */
-	const ssize_t	ring_ofs[0];
+	const uint64_t	ring_ofs[0];
 };
 
 
@@ -536,6 +536,7 @@ struct nmreq {
 	uint32_t	nr_flags;
 	/* various modes, extends nr_ringid */
 	uint32_t	spare2[1];
+	void *nm_mem;
 };
 
 #define NR_REG_MASK		0xf /* values for nr_flags */
-- 
2.7.4

